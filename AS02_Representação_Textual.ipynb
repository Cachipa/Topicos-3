{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4FFrRqY5EPbvN0VMOeDP/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cachipa/Topicos-3/blob/main/AS02_Representa%C3%A7%C3%A3o_Textual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bibliotecas usadas, carrega os dados e preprocessa eles**"
      ],
      "metadata": {
        "id": "E_zOcjAfPb_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from nltk import bigrams\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "import unicodedata\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "import pandas as pd\n",
        "\n",
        "# Carregar os dados\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "\n",
        "# Tokenização\n",
        "tokenized_texts = [doc.split() for doc in newsgroups_train.data]\n",
        "\n",
        "# Os dados utilizados são restringidos por essa variavel porque o colab nao aguenta tudo, para aumentar ou diminuir o numero de textos usados altere o numero abaixo\n",
        "tokenized_texts_first_20 = tokenized_texts[:20]\n",
        "\n",
        "def remove_accents(input_str):\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    return u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
        "\n",
        "string_texts = [' '.join(doc) for doc in tokenized_texts_first_5]\n",
        "\n",
        "# Preprocessamento\n",
        "def preprocess(text):\n",
        "    text = remove_accents(text)\n",
        "    text = re.sub('\\.(?!(\\$[^. ])|\\d)', '', text)\n",
        "    text = re.sub('(?<!\\d)[.;!?\\'\\(\\)#:-](?!\\d)', '', text)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "preprocessed_texts = [preprocess(text) for text in string_texts]"
      ],
      "metadata": {
        "id": "MtpC9wQIAEHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OneHotEncoder**"
      ],
      "metadata": {
        "id": "mcUwGNtJPoi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_vectorizer = CountVectorizer(binary=True)\n",
        "one_hot_encoded = one_hot_vectorizer.fit_transform(preprocessed_texts)\n",
        "np.savetxt('20News_01.txt', one_hot_encoded.toarray(), fmt='%d')"
      ],
      "metadata": {
        "id": "1FECy0pxNPzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Count Vectors**"
      ],
      "metadata": {
        "id": "Xg7nljM-Psb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer()\n",
        "count_vectors = count_vectorizer.fit_transform(preprocessed_texts)\n",
        "terms = count_vectorizer.get_feature_names_out()\n",
        "np.savetxt('20News_02.txt', count_vectors.toarray(), fmt='%d')"
      ],
      "metadata": {
        "id": "Wr0TABOmDEyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF**"
      ],
      "metadata": {
        "id": "QiiW4insQHjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfTransformer()\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(count_vectors)\n",
        "np.savetxt('20News_03.txt', tfidf_vectors.toarray(), fmt='%.3f')"
      ],
      "metadata": {
        "id": "0q75lAd3FAkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**n-grams (2-grams)**"
      ],
      "metadata": {
        "id": "rg1CBlRrQLtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "ngram_vectors = ngram_vectorizer.fit_transform(preprocessed_texts)\n",
        "np.savetxt('20News_04.txt', ngram_vectors.toarray(), fmt='%d')"
      ],
      "metadata": {
        "id": "2tV8o4dsNszq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Co-occurrence Vectors (Context Window = 1)**"
      ],
      "metadata": {
        "id": "xMaWvn8_QO-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "co_occurrence_matrix2 = (count_vectors.T * count_vectors)\n",
        "np.savetxt('20News_05.txt', co_occurrence_matrix2.toarray(), fmt='%d')"
      ],
      "metadata": {
        "id": "nGMs99-DNvjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word2Vec**"
      ],
      "metadata": {
        "id": "UidgsD3fQZJF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fxTTj7jzv7v"
      },
      "outputs": [],
      "source": [
        "word2vec_model = Word2Vec(preprocessed_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
        "word2vec_model.wv.save_word2vec_format('20News_06.txt', binary=False)"
      ]
    }
  ]
}