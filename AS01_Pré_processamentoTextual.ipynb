{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOgGZ6hbuoR/g00JSwbf3u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cachipa/Topicos-3/blob/main/AS01_Pr%C3%A9_processamentoTextual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalização**"
      ],
      "metadata": {
        "id": "7nhSBLMTjmQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import gdown\n",
        "# Download do Shakespeare.txt\n",
        "url = 'https://drive.google.com/file/d/1lBzL2rDT7ksw80ibzkW_1q5XcpdHJcC0/view?usp=drive_link'\n",
        "output = 'Shakespeare.txt'\n",
        "gdown.download( url, output, fuzzy=True)\n",
        "\n",
        "# Carregar o arquivo Shakespeare.txt\n",
        "with open('Shakespeare.txt', 'r', encoding='utf-8') as file:\n",
        "    shakespeare_text = file.read()\n",
        "\n",
        "# Lower case reduction\n",
        "shakespeare_text = shakespeare_text.lower()\n",
        "\n",
        "# Accent and diacritic removal\n",
        "def remove_accents(input_str):\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    return u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
        "\n",
        "shakespeare_text = remove_accents(shakespeare_text)\n",
        "\n",
        "# Canonicalizing of acronyms, currency, date and hyphenated words\n",
        "# Punctuation removal (except currency and date).\n",
        "# Special characters removal\n",
        "nospecial_text = re.sub('\\.(?!(\\$[^. ])|\\d)', '', shakespeare_text)\n",
        "nospecial_text = re.sub('(?<!\\d)[.,;!?\\'\\(\\)#:-](?!\\d)', '', nospecial_text)\n",
        "nospecial_text = re.sub(' +', ' ', nospecial_text)\n",
        "\n",
        "with open('Shakespeare_Normalized.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(nospecial_text)\n"
      ],
      "metadata": {
        "id": "00bLJW9diGmf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8305cca9-c736-40b5-e238-0ec3a4d323d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lBzL2rDT7ksw80ibzkW_1q5XcpdHJcC0\n",
            "To: /content/Shakespeare.txt\n",
            "100%|██████████| 100k/100k [00:00<00:00, 26.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenização**"
      ],
      "metadata": {
        "id": "Bk89dBUBKb1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import TreebankWordTokenizer, wordpunct_tokenize, TweetTokenizer, MWETokenizer\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "import gensim\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "with open('Shakespeare_Normalized.txt', 'r', encoding='utf-8') as file:\n",
        "    normalized_text = file.read()\n",
        "\n",
        "# 01. White Space Tokenization\n",
        "tokenized_whitespace = normalized_text.split()\n",
        "\n",
        "# 02. NLTK: Word Tokenizer\n",
        "nltk_tokenizer = nltk.word_tokenize(normalized_text)\n",
        "\n",
        "# 03. NLTK: Tree Bank Tokenizer\n",
        "treebank_tokenizer = TreebankWordTokenizer().tokenize(normalized_text)\n",
        "\n",
        "# 04. NLTK: Word Punctuation Tokenizer\n",
        "word_punct_tokenizer = wordpunct_tokenize(normalized_text)\n",
        "\n",
        "# 05. NLTK: Tweet Tokenizer\n",
        "tweet_tokenizer = TweetTokenizer().tokenize(normalized_text)\n",
        "\n",
        "# 06. NLTK: MWE Tokenizer\n",
        "mwe_tokenizer = MWETokenizer().tokenize(normalized_text.split())\n",
        "\n",
        "# 07. TextBlob Word Tokenizer\n",
        "textblob_tokenizer = TextBlob(normalized_text).words\n",
        "\n",
        "# 08. spaCy Tokenizer\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(normalized_text)\n",
        "spacy_tokenizer = [token.text for token in doc]\n",
        "\n",
        "# 09. Gensim Word Tokenizer\n",
        "gensim_tokenizer = list(gensim.utils.simple_tokenize(normalized_text))\n",
        "\n",
        "# 10. Keras Tokenization\n",
        "keras_tokenization = text_to_word_sequence(normalized_text)\n",
        "\n",
        "# Salvar os resultados em arquivos separados\n",
        "tokenization_methods = [tokenized_whitespace, nltk_tokenizer, treebank_tokenizer, word_punct_tokenizer,\n",
        "                        tweet_tokenizer, mwe_tokenizer, textblob_tokenizer, spacy_tokenizer,\n",
        "                        gensim_tokenizer, keras_tokenization]\n",
        "file_names = [\"Shakespeare_Normalized_Tokenized{:02d}.txt\".format(i+1) for i in range(len(tokenization_methods))]\n",
        "\n",
        "for method, file_name in zip(tokenization_methods, file_names):\n",
        "    with open(file_name, 'w', encoding='utf-8') as file:\n",
        "        file.write(' '.join(method))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osAdkzCnjyJz",
        "outputId": "c8fd585d-f32f-4278-ebc5-e3f51cb0c85d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stop-words Removal**"
      ],
      "metadata": {
        "id": "1yvWSHG0M9O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "with open('Shakespeare_Normalized_Tokenized02.txt', 'r', encoding='utf-8') as file:\n",
        "    tokenized_text = file.read().split()\n",
        "\n",
        "# Remover stop-words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_text = [word for word in tokenized_text if word.lower() not in stop_words]\n",
        "\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(' '.join(filtered_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vglIqp2k1E4",
        "outputId": "2a16ddf9-abbb-4fd3-afb9-270cfc9ff4f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Lemmatization**"
      ],
      "metadata": {
        "id": "OmqePDKmOgiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord.txt', 'r', encoding='utf-8') as file:\n",
        "    stopword_removed_text = file.read().split()\n",
        "\n",
        "# Lematização\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_text = [lemmatizer.lemmatize(word) for word in stopword_removed_text]\n",
        "\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(' '.join(lemmatized_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXk8NXRglQGc",
        "outputId": "29f1207b-4b90-4008-f95d-4dca961362e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Stemming**"
      ],
      "metadata": {
        "id": "s_R7vHWCPrQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt', 'r', encoding='utf-8') as file:\n",
        "    lemmatized_text = file.read().split()\n",
        "\n",
        "# Aplicar Stemming com Porter Stemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "stemmed_text_porter = [porter_stemmer.stem(word) for word in lemmatized_text]\n",
        "\n",
        "# Aplicar Stemming com Snowball Stemmer\n",
        "snowball_stemmer = SnowballStemmer('english')\n",
        "stemmed_text_snowball = [snowball_stemmer.stem(word) for word in lemmatized_text]\n",
        "\n",
        "# Salvar os resultados em arquivos separados\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming01.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(' '.join(stemmed_text_porter))\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming02.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(' '.join(stemmed_text_snowball))"
      ],
      "metadata": {
        "id": "5kTVwLJ7lhvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análise do Vocabulário**"
      ],
      "metadata": {
        "id": "1dU5zWDKlxQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "def analyze_vocabulary(tokens, filename):\n",
        "\n",
        "    token_counts = Counter(tokens)\n",
        "    token_lengths = {token: len(token) for token in token_counts.keys()}\n",
        "\n",
        "    # Escrevendo os resultados em um arquivo CSV\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['Token', 'Ocorrências', 'Tamanho do Token']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for token, count in token_counts.items():\n",
        "            writer.writerow({'Token': token, 'Ocorrências': count, 'Tamanho do Token': token_lengths[token]})\n",
        "\n",
        "# Carregar o texto lematizado e stemming\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming01.txt', 'r', encoding='utf-8') as file:\n",
        "    porter_stemmed_text = file.read().split()\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming02.txt', 'r', encoding='utf-8') as file:\n",
        "    snowball_stemmed_text = file.read().split()\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt', 'r', encoding='utf-8') as file:\n",
        "    lemmatized_text = file.read().split()\n",
        "\n",
        "# Realizar análise de vocabulário\n",
        "analyze_vocabulary(porter_stemmed_text, 'Shakespeare_Vocabulary_Porter.csv')\n",
        "analyze_vocabulary(snowball_stemmed_text, 'Shakespeare_Vocabulary_Snowball.csv')\n",
        "analyze_vocabulary(lemmatized_text, 'Shakespeare_Vocabulary_Lemmatized.csv')\n",
        "\n",
        "def analyze_csv(filename):\n",
        "    with open(filename, 'r', newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        tokens = []\n",
        "        occurrences = []\n",
        "        token_lengths = []\n",
        "        for row in reader:\n",
        "            tokens.append(row['Token'])\n",
        "            occurrences.append(int(row['Ocorrências']))\n",
        "            token_lengths.append(int(row['Tamanho do Token']))\n",
        "\n",
        "        total_tokens = len(tokens)\n",
        "        avg_occurrences = sum(occurrences) / total_tokens\n",
        "        avg_token_length = sum(token_lengths) / total_tokens\n",
        "\n",
        "        return total_tokens, avg_occurrences, avg_token_length\n",
        "\n",
        "porter_total_tokens, porter_avg_occurrences, porter_avg_length = analyze_csv('Shakespeare_Vocabulary_Porter.csv')\n",
        "snowball_total_tokens, snowball_avg_occurrences, snowball_avg_length = analyze_csv('Shakespeare_Vocabulary_Snowball.csv')\n",
        "lemmatized_total_tokens, lemmatized_avg_occurrences, lemmatized_avg_length = analyze_csv('Shakespeare_Vocabulary_Lemmatized.csv')\n",
        "\n",
        "# Escrevendo o arquivo final\n",
        "with open('Shakespeare_Vocabulary_Analysis.txt', 'w', encoding='utf-8') as analysis_file:\n",
        "    analysis_file.write(\"Comparação de Análise de Vocabulário\\n\\n\")\n",
        "    analysis_file.write(\"Método de Stemming: Porter\\n\")\n",
        "    analysis_file.write(f\"Tamanho do Vocabulário: {porter_total_tokens}\\n\")\n",
        "    analysis_file.write(f\"Número Médio de Ocorrências: {porter_avg_occurrences:.2f}\\n\")\n",
        "    analysis_file.write(f\"Tamanho Médio dos Tokens: {porter_avg_length:.2f}\\n\\n\")\n",
        "\n",
        "    analysis_file.write(\"Método de Stemming: Snowball\\n\")\n",
        "    analysis_file.write(f\"Tamanho do Vocabulário: {snowball_total_tokens}\\n\")\n",
        "    analysis_file.write(f\"Número Médio de Ocorrências: {snowball_avg_occurrences:.2f}\\n\")\n",
        "    analysis_file.write(f\"Tamanho Médio dos Tokens: {snowball_avg_length:.2f}\\n\\n\")\n",
        "\n",
        "    analysis_file.write(\"Método de Lematização\\n\")\n",
        "    analysis_file.write(f\"Tamanho do Vocabulário: {lemmatized_total_tokens}\\n\")\n",
        "    analysis_file.write(f\"Número Médio de Ocorrências: {lemmatized_avg_occurrences:.2f}\\n\")\n",
        "    analysis_file.write(f\"Tamanho Médio dos Tokens: {lemmatized_avg_length:.2f}\\n\")"
      ],
      "metadata": {
        "id": "zG7K8m0Klzf2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}